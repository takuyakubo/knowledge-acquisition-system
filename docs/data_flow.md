# データフロー

## 概要

情報収集・知識管理サブシステム内のデータは、収集から可視化までの一連のプロセスを経て流れていきます。本ドキュメントではそのデータフローを説明します。

## 基本データフロー

```
外部ソース → 収集 → 前処理 → 知識抽出 → ベクトル化 → インデキシング → 検索/可視化
```

### 1. データ収集フェーズ

外部ソース（論文DB、Web、ローカルファイル）からデータを収集します。

**入力**: ソースURL、API呼び出し、ファイルパス
**出力**: 収集された生データ、メタデータ
**保存先**: ローカルファイルシステム（一時ストレージ）

### 2. 前処理フェーズ

収集データのクリーニング、構造化、メタデータ抽出を行います。

**入力**: 収集された生データ
**出力**: 構造化テキスト、抽出メタデータ
**保存先**: ローカルファイルシステム（処理済みデータ）、SQLデータベース（メタデータ）

### 3. 知識抽出フェーズ

前処理済みテキストから要約、キーワード、エンティティなどを抽出します。

**入力**: 構造化テキスト
**出力**: 要約、キーワード、エンティティリスト
**保存先**: SQLデータベース（構造化知識）

### 4. ベクトル化フェーズ

テキストをベクトル表現に変換します。

**入力**: 構造化テキスト、抽出された知識
**出力**: ベクトル埋め込み
**保存先**: ベクトルデータベース（FAISS, Chroma等）

### 5. 知識グラフ構築フェーズ

エンティティ間の関係を抽出し、グラフを構築します。

**入力**: 抽出エンティティ、テキストコンテンツ
**出力**: エンティティ関係グラフ
**保存先**: グラフデータベース（Neo4j等）

### 6. 検索インデックス更新フェーズ

検索を効率化するためのインデックスを更新します。

**入力**: ベクトル、メタデータ、キーワード
**出力**: 検索インデックス
**保存先**: ベクトルDB、検索エンジン

### 7. API応答フェーズ

他サブシステムやユーザーからのクエリに応答します。

**入力**: 検索クエリ、グラフクエリ
**出力**: 検索結果、グラフデータ
**送信先**: APIクライアント、UI

## データ変換の詳細

### テキスト → 構造化テキスト

- HTMLタグ除去
- セクション分割
- テーブル/リスト構造の抽出

### 構造化テキスト → 知識

- LLMを用いた要約生成
- キーワード抽出（TF-IDF, LLM）
- エンティティ認識（spaCy, LLM）

### テキスト → ベクトル

- 埋め込みモデル（OpenAI, HuggingFace）によるベクトル化
- セクション/段落単位での分割と埋め込み

### エンティティ → 関係グラフ

- テキスト内のエンティティ関係抽出
- 関係タイプの分類
- グラフノード・エッジの作成

## データストレージの関連性

各ストレージシステムは以下のように連携します：

- **ファイルシステム** → 生データと処理済みテキスト
- **SQLデータベース** → メタデータと構造化知識
- **ベクトルデータベース** → 検索用ベクトル
- **グラフデータベース** → エンティティ関係

これらのストレージは一意のIDによって相互参照され、統合されたデータアクセスを可能にします。